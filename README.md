# ğŸ§  T5 Knowledge Distillation for Summarization

This project demonstrates how to perform knowledge distillation from a fineâ€tuned **T5â€‘Base** model to a lightweight **T5â€‘Small** model using the [News Summary Dataset](https://www.kaggle.com/datasets/sunnysai12345/news-summary).

---

## ğŸ“˜ Project Overview

This notebook shows how to:

1. **Distill** a fineâ€‘tuned T5â€‘Base teacher into a T5â€‘Small student using KL Divergence and Crossâ€‘Entropy  
2. **Evaluate** both models using **ROUGE** and **BERTScore**  
3. Report **knowledge retention** by comparing student scores relative to teacher scores  

> âœ… Our goal is not to maximize absolute performance but to observe **knowledge retention** through distillation.

---

## ğŸ§  Model Setup & Important Notes

- **Teacher model**: `mrm8488/t5-base-finetuned-summarize-news` â€” already **fineâ€‘tuned** on this news summarization task (we did **not** fineâ€‘tune it ourselves).  
- **Student model**: `google-t5/t5-small` â€” used **without** additional fineâ€‘tuning.  
- This configuration follows the [PyTorch Torchtune KD tutorial](https://pytorch.org/torchtune/0.3/tutorials/llama_kd_tutorial.html), which shows that transferring knowledge from a specialized teacher into a nonâ€‘fineâ€‘tuned student best highlights the effect of distillation.  
- âš ï¸ Teacher performance (ROUGE, BERTScore) depends entirely on the original fineâ€‘tuning quality by the model uploader.

---

## ğŸ“¥ Dataset

We use the **News Summary Dataset** from Kaggle, downloaded via `kagglehub`:

- `news_summary.csv` â€” main file with `text` (article) and `ctext` (summary) columns. Used for both distillation and evaluation.  
- `news_summary_more.csv` â€” additional raw articles (not used directly in this notebook).

---

## ğŸ“‚ Repository Contents

- `KD.ipynb` â€” Colabâ€‘compatible notebook containing:  
  - Dataset download & exploration  
  - Knowledge distillation training loop  
  - Evaluation (ROUGE, BERTScore, retention)  

---

## ğŸš€ How to Use

Run all cells in order:

1. **Download dataset** via `kagglehub`  
2. **Load & filter data**  
3. **Distill** T5â€‘Base â†’ T5â€‘Small  
4. **Evaluate** models and compute retention  

---

## ğŸ“Š Metrics Used

- **ROUGEâ€‘1, ROUGEâ€‘2, ROUGEâ€‘L** â€” nâ€‘gram overlap with reference summaries  
- **BERTScore (F1)** using `roberta-large` â€” semantic similarity  
- **Retention (%)** = (student_score / teacher_score) Ã—Â 100

---

## âš ï¸ Notebook Rendering on GitHub

The Jupyter notebook in this repository currently **wonâ€™t render properly** on GitHub due to a known issue (â€œstateâ€ key missing in `metadata.widgets`). For details, see [GitHub Discussion #155944](https://github.com/orgs/community/discussions/155944).

ğŸ“¥ **Workaround:** Download the notebook (`KD.ipynb`) and open it locally in Jupyter or Google Colab to view all cells and outputs correctly.


